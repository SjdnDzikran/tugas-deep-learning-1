{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Implementation Assignment",
    "",
    "Notebook ini mengimplementasikan jaringan syaraf tiruan untuk memprediksi risiko stroke menggunakan dataset *Stroke Prediction* dari Kaggle. Seluruh komponen utama (aktivasi, loss, forward pass, backward pass, dan algoritma pelatihan) diimplementasikan dari awal dengan `NumPy` untuk memenuhi persyaratan tugas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persiapan Data",
    "",
    "1. Unduh berkas **`healthcare-dataset-stroke-data.csv`** dari [Stroke Prediction Dataset di Kaggle](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset).",
    "2. Letakkan berkas tersebut pada direktori root repository ini dan ubah namanya menjadi **`stroke-data.csv`** (atau sesuaikan variabel `DATA_PATH`).",
    "3. Jalankan sel berikut untuk memuat dan meninjau data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('stroke-data.csv')\n",
    "\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        'Dataset tidak ditemukan. Unduh `healthcare-dataset-stroke-data.csv` dari Kaggle, '\n",
    "        'letakkan sebagai `stroke-data.csv` pada direktori root proyek ini, '\n",
    "        'lalu jalankan kembali notebook.'\n",
    "    )\n",
    "\n",
    "raw_df = pd.read_csv(DATA_PATH)\n",
    "print(f'Jumlah sampel: {len(raw_df):,}')\n",
    "raw_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pembersihan & Pra-pemrosesan",
    "",
    "* Mengisi nilai BMI yang hilang dengan median.",
    "* Menghapus kategori gender langka (`Other`).",
    "* Melakukan one-hot encoding pada fitur kategorikal.",
    "* Menskalakan fitur numerik menggunakan `StandardScaler`.",
    "* Membagi data menjadi train dan test dengan proporsi 80:20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = raw_df.copy()\n",
    "\n",
    "# Tangani nilai yang hilang dan kategori langka\n",
    "if 'bmi' in df.columns:\n",
    "    df['bmi'] = df['bmi'].fillna(df['bmi'].median())\n",
    "if 'gender' in df.columns:\n",
    "    df = df[df['gender'] != 'Other']\n",
    "\n",
    "categorical_cols = [\n",
    "    'gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status'\n",
    "]\n",
    "existing_categoricals = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "df = pd.get_dummies(df, columns=existing_categoricals, drop_first=True)\n",
    "\n",
    "target_col = 'stroke'\n",
    "feature_cols = [col for col in df.columns if col != target_col]\n",
    "\n",
    "X = df[feature_cols].to_numpy(dtype=float)\n",
    "y = df[target_col].to_numpy(dtype=float).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(f'Jumlah fitur setelah encoding: {input_dim}')\n",
    "print(f'Proporsi kelas positif di train: {y_train.mean():.3f}')\n",
    "print(f'Proporsi kelas positif di test : {y_test.mean():.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fungsi Aktivasi dan Loss (From Scratch)",
    "",
    "Tiga fungsi aktivasi non-linear yang umum digunakan diuji pada hidden layer: **Sigmoid**, **Tanh**, dan **ReLU**. Lapisan output menggunakan sigmoid karena tugasnya adalah klasifikasi biner. Loss function yang dipakai adalah **Binary Cross-Entropy (BCE)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Array = np.ndarray\n",
    "\n",
    "\n",
    "def sigmoid(z: Array) -> Array:\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def tanh(z: Array) -> Array:\n",
    "    return np.tanh(z)\n",
    "\n",
    "\n",
    "def relu(z: Array) -> Array:\n",
    "    return np.maximum(0.0, z)\n",
    "\n",
    "\n",
    "def leaky_relu(z: Array, alpha: float = 0.01) -> Array:\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "\n",
    "\n",
    "def sigmoid_backward(a: Array, _: Array) -> Array:\n",
    "    return a * (1.0 - a)\n",
    "\n",
    "\n",
    "def tanh_backward(a: Array, _: Array) -> Array:\n",
    "    return 1.0 - np.square(a)\n",
    "\n",
    "\n",
    "def relu_backward(_: Array, z: Array) -> Array:\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "\n",
    "def leaky_relu_backward(_: Array, z: Array, alpha: float = 0.01) -> Array:\n",
    "    grad = np.ones_like(z) * alpha\n",
    "    grad[z > 0] = 1.0\n",
    "    return grad\n",
    "\n",
    "\n",
    "activation_functions: Dict[str, Dict[str, callable]] = {\n",
    "    'sigmoid': {'forward': sigmoid, 'backward': sigmoid_backward},\n",
    "    'tanh': {'forward': tanh, 'backward': tanh_backward},\n",
    "    'relu': {'forward': relu, 'backward': relu_backward},\n",
    "    'leaky_relu': {\n",
    "        'forward': lambda z: leaky_relu(z, alpha=0.01),\n",
    "        'backward': lambda a, z: leaky_relu_backward(a, z, alpha=0.01),\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true: Array, y_pred: Array, eps: float = 1e-12) -> float:\n",
    "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    loss = -(\n",
    "        y_true * np.log(y_pred) + (1.0 - y_true) * np.log(1.0 - y_pred)\n",
    "    )\n",
    "    return float(np.mean(loss))\n",
    "\n",
    "\n",
    "def binary_cross_entropy_derivative(y_true: Array, y_pred: Array, eps: float = 1e-12) -> Array:\n",
    "    y_pred = np.clip(y_pred, eps, 1.0 - eps)\n",
    "    return -(y_true / y_pred) + ((1.0 - y_true) / (1.0 - y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Pass, Backward Pass, dan Optimisasi",
    "",
    "Bagian ini mendefinisikan fungsi utilitas untuk:",
    "",
    "* Inisialisasi bobot (Xavier/Glorot uniform).",
    "* Melakukan forward pass.",
    "* Melakukan backward pass dan menghitung gradien.",
    "* Memperbarui bobot dengan Gradient Descent.",
    "* Menghasilkan prediksi probabilitas dan kelas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_sizes: List[int], rng: np.random.Generator) -> List[Dict[str, Array]]:\n",
    "    parameters: List[Dict[str, Array]] = []\n",
    "    for n_in, n_out in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "        limit = math.sqrt(6.0 / (n_in + n_out))\n",
    "        W = rng.uniform(-limit, limit, size=(n_out, n_in))\n",
    "        b = np.zeros(n_out)\n",
    "        parameters.append({'W': W, 'b': b})\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def forward_pass(X: Array, params: List[Dict[str, Array]], activation_sequence: List[str]) -> Tuple[Array, List[Dict[str, Array]]]:\n",
    "    A = X\n",
    "    caches: List[Dict[str, Array]] = []\n",
    "    for layer_idx, (layer, activation_name) in enumerate(zip(params, activation_sequence)):\n",
    "        W, b = layer['W'], layer['b']\n",
    "        Z = A @ W.T + b\n",
    "        activation = activation_functions[activation_name]['forward']\n",
    "        A_next = activation(Z)\n",
    "        caches.append({'A_prev': A, 'Z': Z, 'A': A_next, 'activation': activation_name})\n",
    "        A = A_next\n",
    "    return A, caches\n",
    "\n",
    "\n",
    "def backward_pass(\n",
    "    y_true: Array,\n",
    "    caches: List[Dict[str, Array]],\n",
    "    params: List[Dict[str, Array]],\n",
    "    activation_sequence: List[str],\n",
    "    loss_derivative=binary_cross_entropy_derivative,\n",
    ") -> List[Dict[str, Array]]:\n",
    "    grads: List[Dict[str, Array]] = [None] * len(params)\n",
    "    m = y_true.shape[0]\n",
    "    dA = loss_derivative(y_true, caches[-1]['A'])\n",
    "\n",
    "    for idx in reversed(range(len(params))):\n",
    "        cache = caches[idx]\n",
    "        activation_name = activation_sequence[idx]\n",
    "        activation_grad = activation_functions[activation_name]['backward']\n",
    "\n",
    "        dZ = dA * activation_grad(cache['A'], cache['Z'])\n",
    "        A_prev = cache['A_prev']\n",
    "        dW = (dZ.T @ A_prev) / m\n",
    "        db = np.mean(dZ, axis=0)\n",
    "        grads[idx] = {'dW': dW, 'db': db}\n",
    "\n",
    "        if idx != 0:\n",
    "            dA = dZ @ params[idx]['W']\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def update_parameters(\n",
    "    params: List[Dict[str, Array]], grads: List[Dict[str, Array]], learning_rate: float\n",
    ") -> None:\n",
    "    for layer, grad in zip(params, grads):\n",
    "        layer['W'] -= learning_rate * grad['dW']\n",
    "        layer['b'] -= learning_rate * grad['db']\n",
    "\n",
    "\n",
    "def predict_proba(\n",
    "    X: Array, params: List[Dict[str, Array]], activation_sequence: List[str]\n",
    ") -> Array:\n",
    "    probs, _ = forward_pass(X, params, activation_sequence)\n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_classes(probs: Array, threshold: float = 0.5) -> Array:\n",
    "    return (probs >= threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(\n",
    "    X: Array,\n",
    "    y: Array,\n",
    "    strategy: str,\n",
    "    batch_size: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> Iterable[Tuple[Array, Array]]:\n",
    "    indices = np.arange(X.shape[0])\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    if strategy == 'batch':\n",
    "        yield X[indices], y[indices]\n",
    "    elif strategy == 'stochastic':\n",
    "        for idx in indices:\n",
    "            yield X[idx: idx + 1], y[idx: idx + 1]\n",
    "    elif strategy == 'mini-batch':\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch_idx = indices[start:start + batch_size]\n",
    "            yield X[batch_idx], y[batch_idx]\n",
    "    else:\n",
    "        raise ValueError(f'Strategi tidak dikenal: {strategy}')\n",
    "\n",
    "\n",
    "def accuracy_score(y_true: Array, probs: Array, threshold: float = 0.5) -> float:\n",
    "    preds = predict_classes(probs, threshold)\n",
    "    return float(np.mean(preds == y_true))\n",
    "\n",
    "\n",
    "def compute_classification_metrics(\n",
    "    y_true: Array, probs: Array, threshold: float = 0.5\n",
    ") -> Dict[str, float]:\n",
    "    preds = predict_classes(probs, threshold)\n",
    "    y_true = y_true.astype(int)\n",
    "    preds = preds.astype(int)\n",
    "\n",
    "    tp = float(np.sum((preds == 1) & (y_true == 1)))\n",
    "    tn = float(np.sum((preds == 0) & (y_true == 0)))\n",
    "    fp = float(np.sum((preds == 1) & (y_true == 0)))\n",
    "    fn = float(np.sum((preds == 0) & (y_true == 1)))\n",
    "\n",
    "    eps = 1e-12\n",
    "    precision = tp / (tp + fp + eps)\n",
    "    recall = tp / (tp + fn + eps)\n",
    "    f1 = 2 * precision * recall / (precision + recall + eps)\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn + eps)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "\n",
    "\n",
    "def train_network(\n",
    "    X: Array,\n",
    "    y: Array,\n",
    "    layer_sizes: List[int],\n",
    "    hidden_activation: str,\n",
    "    output_activation: str = 'sigmoid',\n",
    "    strategy: str = 'batch',\n",
    "    learning_rate: float = 0.01,\n",
    "    epochs: int = 200,\n",
    "    batch_size: int = 32,\n",
    "    random_state: int = 42,\n",
    "    X_val: Optional[Array] = None,\n",
    "    y_val: Optional[Array] = None,\n",
    ") -> Tuple[List[Dict[str, Array]], Dict[str, List[float]], List[str]]:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    params = initialize_parameters(layer_sizes, rng)\n",
    "    activation_sequence = [hidden_activation] * (len(layer_sizes) - 2) + [output_activation]\n",
    "\n",
    "    history: Dict[str, List[float]] = {\n",
    "        'train_loss': [],\n",
    "        'train_accuracy': [],\n",
    "    }\n",
    "    if X_val is not None and y_val is not None:\n",
    "        history['val_loss'] = []\n",
    "        history['val_accuracy'] = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for X_batch, y_batch in iterate_batches(X, y, strategy, batch_size, rng):\n",
    "            probs, caches = forward_pass(X_batch, params, activation_sequence)\n",
    "            grads = backward_pass(y_batch, caches, params, activation_sequence)\n",
    "            update_parameters(params, grads, learning_rate)\n",
    "\n",
    "        train_probs = predict_proba(X, params, activation_sequence)\n",
    "        history['train_loss'].append(binary_cross_entropy(y, train_probs))\n",
    "        history['train_accuracy'].append(accuracy_score(y, train_probs))\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            val_probs = predict_proba(X_val, params, activation_sequence)\n",
    "            history['val_loss'].append(binary_cross_entropy(y_val, val_probs))\n",
    "            history['val_accuracy'].append(accuracy_score(y_val, val_probs))\n",
    "\n",
    "    return params, history, activation_sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eksperimen: Kombinasi Aktivasi dan Strategi Gradient Descent",
    "",
    "Eksperimen dilakukan pada arsitektur jaringan `[input_dim, 32, 16, 1]` dengan laju belajar konstan. Kombinasi yang diuji:",
    "",
    "* Fungsi aktivasi hidden layer: **Sigmoid**, **Tanh**, **ReLU**.",
    "* Strategi gradient descent: **Batch**, **Mini-batch (32)**, dan **Stochastic**.",
    "",
    "Metode evaluasi menggunakan akurasi, presisi, recall, dan F1-score pada data uji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_sizes = [input_dim, 32, 16, 1]\n",
    "activations_to_test = ['sigmoid', 'tanh', 'relu']\n",
    "strategies = {\n",
    "    'batch': None,\n",
    "    'mini-batch': 32,\n",
    "    'stochastic': 1,\n",
    "}\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 200\n",
    "\n",
    "experiment_results: List[Dict[str, float]] = []\n",
    "training_histories: Dict[Tuple[str, str], Dict[str, List[float]]] = {}\n",
    "trained_models: Dict[Tuple[str, str], Tuple[List[Dict[str, Array]], List[str]]] = {}\n",
    "\n",
    "for activation_name in activations_to_test:\n",
    "    for strategy_name, batch_value in strategies.items():\n",
    "        batch_size = 1 if strategy_name == 'stochastic' else (batch_value or X_train.shape[0])\n",
    "        params, history, activation_sequence = train_network(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            layer_sizes,\n",
    "            hidden_activation=activation_name,\n",
    "            strategy='stochastic' if strategy_name == 'stochastic' else strategy_name,\n",
    "            learning_rate=learning_rate,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size if strategy_name != 'batch' else X_train.shape[0],\n",
    "            random_state=42,\n",
    "        )\n",
    "\n",
    "        train_probs = predict_proba(X_train, params, activation_sequence)\n",
    "        test_probs = predict_proba(X_test, params, activation_sequence)\n",
    "\n",
    "        train_loss = binary_cross_entropy(y_train, train_probs)\n",
    "        test_loss = binary_cross_entropy(y_test, test_probs)\n",
    "        test_metrics = compute_classification_metrics(y_test, test_probs)\n",
    "\n",
    "        experiment_results.append({\n",
    "            'hidden_activation': activation_name,\n",
    "            'gradient_strategy': strategy_name,\n",
    "            'learning_rate': learning_rate,\n",
    "            'epochs': epochs,\n",
    "            'batch_size': batch_size,\n",
    "            'train_loss': train_loss,\n",
    "            'test_loss': test_loss,\n",
    "            'test_accuracy': test_metrics['accuracy'],\n",
    "            'test_precision': test_metrics['precision'],\n",
    "            'test_recall': test_metrics['recall'],\n",
    "            'test_f1': test_metrics['f1'],\n",
    "        })\n",
    "\n",
    "        training_histories[(activation_name, strategy_name)] = history\n",
    "        trained_models[(activation_name, strategy_name)] = (params, activation_sequence)\n",
    "\n",
    "        print(\n",
    "            f\"Aktivasi: {activation_name:<7} | Strategi: {strategy_name:<10} | \"\n",
    "            f\"Train Loss: {train_loss:.4f} | Test F1: {test_metrics['f1']:.4f}\"\n",
    "        )\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "results_df.sort_values('test_f1', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ringkasan Evaluasi",
    "",
    "Tabel berikut membandingkan performa setiap kombinasi fungsi aktivasi dan strategi gradient descent pada data uji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.sort_values('test_f1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perilaku Training",
    "",
    "Grafik berikut memperlihatkan dinamika loss dan akurasi selama proses training untuk beberapa kombinasi terpilih."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "selected_combinations = [\n",
    "    ('sigmoid', 'batch'),\n",
    "    ('tanh', 'mini-batch'),\n",
    "    ('relu', 'stochastic'),\n",
    "]\n",
    "\n",
    "for activation_name, strategy_name in selected_combinations:\n",
    "    history = training_histories.get((activation_name, strategy_name))\n",
    "    if history is None:\n",
    "        continue\n",
    "    axes[0].plot(history['train_loss'], label=f'{activation_name} / {strategy_name}')\n",
    "    axes[1].plot(history['train_accuracy'], label=f'{activation_name} / {strategy_name}')\n",
    "\n",
    "axes[0].set_title('Training Loss per Epoch')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].set_title('Training Accuracy per Epoch')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretasi",
    "",
    "* **Arsitektur**: Model tiga lapis `[input_dim, 32, 16, 1]` memberikan kapasitas yang cukup untuk mempelajari pola pada fitur numerik dan kategorikal yang telah diencoding.",
    "* **Aktivasi**: Sigmoid dan Tanh cenderung stabil pada dataset yang tidak terlalu besar, sementara ReLU dapat mengalami konvergensi yang lebih cepat namun memerlukan perhatian terhadap laju belajar untuk menghindari neuron mati.",
    "* **Strategi Gradient Descent**: Mini-batch sering kali memberikan kompromi terbaik antara stabilitas gradien (batch) dan kecepatan pembaruan (stochastic). Pengamatan pada loss/akurasi membantu memahami dinamika training masing-masing strategi.",
    "* **Metode Evaluasi**: Karena dataset tidak seimbang, F1-score dan recall penting untuk memonitor performa model mendeteksi kasus stroke yang jarang.",
    "",
    "Eksperimen lanjutan dapat mencakup penyesuaian laju belajar adaptif, penyeimbangan kelas, atau penambahan regularisasi untuk meningkatkan generalisasi."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}